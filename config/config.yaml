# configs/mnist_config.yaml

description: "Training a recursive CNN on various classification datasets using a DDP framework NOT pretrained."

seed: 42 # Random seed for reproducibility

# Model configuration
# Note: in_channels and num_classes will be automatically set based on the dataset
model:
  name: "ResNet"  # Options: SimpleCNN, ResNet
  # in_channels: auto  # Will be set automatically based on dataset
  # num_classes: auto  # Will be set automatically based on dataset
  pretrained: false  # Whether to use pretrained weights (if applicable)

# Dataset configuration
data:
  dataset_name: "CIFAR100"  # Options: MNIST, FashionMNIST, CIFAR10, CIFAR100, SVHN, STL10, KMNIST
  path: "/scratch/narjis/"  # Path to download/load datasets
  val_split_size: 0.1  # 10% of training data for validation
  augmentation: false  # Whether to apply data augmentation during training
  dataloader_workers:
    train: 8
    val: 8
    test: 8
  use_precomputed_features: false
  precomputed_path: "/scratch/narjis"
# Recursion hyperparameters
recursion:
  recursive_mode: false
  N_supervision_steps:  16  # The outer loop for deep supervision (from pseudo-code) should be 1 if recursive_mode is false !!!!!
  N_latent_steps: 6  # The 'n' in latent_recursion
  N_deep_steps: 3  # The 'T' in deep_recursion
  init_strategy: "zeros"  # "zeros" or "random": init_strategy of latent and output embeddings

# Training parameters
training:
  epochs: 100
  batch_size: 128  # Per GPU batch size
  learning_rate: 1.0e-4
  weight_decay: 0
  scale_lr_by_world_size: true  # If true, effective_lr = learning_rate * world_size
  lr_scheduler_type: "step"  # "cosine", "step", "none"
  warmup_steps: 0
  gradient_accumulation_steps: 1  # Note: DDP + per-step .backward() means grad_accum is complex. Keeping at 1.
  early_stopping:
    enabled: true
    patience: 10
    metric: "val_accuracy"  # or "val_loss"
    mode: "max"  # "min" for loss, "max" for accuracy

# Testing (Inference) parameters
testing:
  N_supervision_steps: 1  # Number of supervision steps for inference

# Distributed Data Parallel (DDP) configuration
ddp:
  backend: "nccl"  # 'nccl' for NVIDIA GPUs, 'gloo' for CPU

# Saving configuration
saving:
  base_output_dir: "./experiments"  # Base directory for all experiments
  save_only_on_rank_0: true
  best_model_name: "best_model.pth"

# Example configurations for different datasets:
# 
# For CIFAR10:
#   data:
#     dataset_name: "CIFAR10"
#   training:
#     epochs: 100
#     learning_rate: 1.0e-3
#
# For CIFAR100:
#   data:
#     dataset_name: "CIFAR100"
#   training:
#     epochs: 150
#     learning_rate: 1.0e-3
#
# For FashionMNIST:
#   data:
#     dataset_name: "FashionMNIST"
#   training:
#     epochs: 30
#
# For SVHN:
#   data:
#     dataset_name: "SVHN"
#   training:
#     epochs: 50